{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mt\n",
    "import itertools as it\n",
    "import Funzioni as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=pd.read_csv(\"C:/Users/Gulli/Desktop/Uni/Magi/MachineLearningandstat/Progetto_ML/your_dataset.csv\")\n",
    "X=ds.iloc[:,:-1]\n",
    "Y=ds.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.isnull().sum())\n",
    "ds = ds.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pui scrivere funzione\n",
    "Q1 = X.quantile(0.25)\n",
    "Q3 = X.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "# Filtra i dati rimuovendo gli outlier\n",
    "X_cleaned = X[~((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "Y_cleaned=Y[X_cleaned.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count1=np.sum(Y_cleaned==1)\n",
    "count_inv=np.sum(Y_cleaned==-1)\n",
    "print(f'{count1}, {count_inv},{Y_cleaned.shape-count_inv}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cleaned.reset_index(drop=True, inplace=True)\n",
    "Y_cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_cleaned.sample(frac=0.7, random_state=42)  # Estrae il 70% dei dati in modo casuale\n",
    "test = X_cleaned.drop(train.index)\n",
    "X_test=test.values\n",
    "X_train=train.values\n",
    "Y_train=Y_cleaned.loc[train.index].values\n",
    "Y_test=Y_cleaned.loc[test.index].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#puoi scrivere funzione\n",
    "train_mean = X_train.mean(axis=0)\n",
    "train_std = X_train.std(axis=0)\n",
    "# Standardizza il training set\n",
    "X_train_standardized = (X_train - train_mean) / train_std\n",
    "# Usa la stessa media e std per standardizzare il test set\n",
    "X_test_standardized = (X_test - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUNING\n",
    "perceptron_param = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 1],\n",
    "    'n': [50, 100, 200, 500],\n",
    "}\n",
    "\n",
    "pegasos_param = {\n",
    "    'lambda_par': [0.001, 0.01, 0.1, 1],\n",
    "    'n': [50, 100, 200, 500],\n",
    "}\n",
    "\n",
    "logistic_param = {\n",
    "    'lambda_par': [0.001, 0.01, 0.1, 1],\n",
    "    'n': [50, 100, 200, 500],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_perc_in, param_perc_in = fn.tuning_par(fn.Perceptron, pd.DataFrame(X_train_standardized), pd.Series(Y_train), perceptron_param ,5)\n",
    "loss_pegasos_in, param_pegasos_in = fn.tuning_par(fn.PegasosSVM, pd.DataFrame(X_train_standardized), pd.Series(Y_train), pegasos_param, 5)\n",
    "loss_logit_in, param_logit_in = fn.tuning_par(fn.RegLogisticClass, pd.DataFrame(X_train_standardized), pd.Series(Y_train), logistic_param, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(param_perc_in, loss_perc_in)\n",
    "print(param_pegasos_in, loss_pegasos_in)\n",
    "print(param_logit_in, loss_logit_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = X_cleaned.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr = correlation_matrix[(correlation_matrix > 0.9) | (correlation_matrix < -0.9)]\n",
    "high_corr = high_corr[high_corr != 1.0]\n",
    "high_corr.stack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#puoi scrivere funzione\n",
    "np.random.seed(42)\n",
    "\n",
    "for i in [2,5,9,[5,9],[2,5],[2,9],[2,5,9]]:\n",
    "    X_reduced_train=np.delete(X_train_standardized, i, axis=1)\n",
    "    X_reduced_test=np.delete(X_test_standardized, i, axis=1)\n",
    "    perceptron_model = fn.Perceptron(0.1,500)\n",
    "    \n",
    "    perceptron_model.fit(X_reduced_train, Y_train)\n",
    "    \n",
    "    y_pred_perceptron_tr = perceptron_model.predict(X_reduced_train)\n",
    "    y_pred_perceptron_test = perceptron_model.predict(X_reduced_test)\n",
    "   \n",
    "    loss_perceptron_tr = fn.Zero_One_Loss(Y_train, y_pred_perceptron_tr,Y_train.shape[0])\n",
    "    loss_perceptron_test = fn.Zero_One_Loss(Y_test, y_pred_perceptron_test,Y_test.shape[0])\n",
    "    print(f\"0-1 Loss Perceptron (Training): {loss_perceptron_tr}  \")\n",
    "    print(f\"0-1 Loss Perceptron (Test): {loss_perceptron_test}  \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#togliamo gli stessi per tutti!\n",
    "X_train=np.delete(X_train_standardized, [5,9], axis=1)\n",
    "X_test=np.delete(X_test_standardized, [5,9], axis=1) #per logistic è la peggiore(però test mejo di training), per pegasos migliore, perceptron  migliore\n",
    "#X_train_pegasos=np.delete(X_train, 2, axis=1)\n",
    "#X_test_pegasos=np.delete(X_test, 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_perc, param_perc = fn.tuning_par(fn.Perceptron, pd.DataFrame(X_train), pd.Series(Y_train), perceptron_param ,5)\n",
    "loss_pegasos, param_pegasos = fn.tuning_par(fn.PegasosSVM, pd.DataFrame(X_train), pd.Series(Y_train), pegasos_param, 5)\n",
    "loss_logit, param_logit = fn.tuning_par(fn.RegLogisticClass, pd.DataFrame(X_train), pd.Series(Y_train), logistic_param, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(param_perc)\n",
    "print(param_pegasos)\n",
    "print(param_logit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METTI COME INPUT COLUMN NAMES CHE SE NO FA X1-X8 E NON TOGLIE LE VARIABILE CHE ABBIAMO TOLTO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10']\n",
      "['x1' 'x2' 'x3' 'x4' 'x5' 'x7' 'x8' 'x9']\n"
     ]
    }
   ],
   "source": [
    "column_names = [f'x{i+1}' for i in range(X.shape[1])]\n",
    "column_names=np.delete(column_names, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly_train, X_poly_test = fn.Polynomial_exp(X_train,X_test, column_names)\n",
    "#X_poly_pega_train, X_poly_pega_test = fn.Polynomial_exp(X_train_pegasos,X_test_pegasos) , per semplicità uso stesso dataset per tutti e 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEVO FARE TUNING DI NUOVO PER TUTTI CON POL EXP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_pegasos, param_pegasos = fn.tuning_par(fn.RegLogisticClass, X_poly_train, pd.Series(Y_train), pegasos_param, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_pegasos #cambia tra X_train e X_poly_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RINORMALIZZA I DATASET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean_poly = X_poly_train.mean(axis=0)\n",
    "train_std_poly = X_poly_train.std(axis=0)\n",
    "# Standardizza il training set\n",
    "X_train_poly_standardized = (X_poly_train - train_mean_poly) / train_std_poly\n",
    "# Usa la stessa media e std per standardizzare il test set\n",
    "X_test_poly_standardized = (X_poly_test - train_mean_poly) / train_std_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_model=fn.Perceptron(0.1,50)\n",
    "pegasos_model=fn.PegasosSVM(0.01,500)\n",
    "logistic_model=fn.RegLogisticClass(1,100)\n",
    "perceptron_model.fit(X_train_poly_standardized.values, Y_train)\n",
    "pegasos_model.fit(X_train_poly_standardized.values, Y_train)\n",
    "logistic_model.fit(X_train_poly_standardized.values, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_perceptron = perceptron_model.weights\n",
    "weights_pegasos = pegasos_model.weights\n",
    "weights_logistic = logistic_model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store and compare the weights\n",
    "weights_df = pd.DataFrame({\n",
    "    'Feature': X_train_poly_standardized.columns,\n",
    "    'Perceptron': weights_perceptron,\n",
    "    'Pegasos': weights_pegasos,\n",
    "    'Logistic_Regression': weights_logistic\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_poly_standardized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m correlation_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mX_train_poly_standardized\u001b[49m\u001b[38;5;241m.\u001b[39mcorr()\n\u001b[0;32m      2\u001b[0m correlation_matrix\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_poly_standardized' is not defined"
     ]
    }
   ],
   "source": [
    "correlation_matrix = X_train_poly_standardized.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr = correlation_matrix[(correlation_matrix > 0.9) | (correlation_matrix < -0.9)]\n",
    "high_corr = high_corr[high_corr != 1.0]\n",
    "high_corr.stack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)  # Mostra tutte le righe\n",
    "pd.set_option('display.max_columns', None)  # Mostra tutte le colonne\n",
    "pd.set_option('display.width', None)  # Adatta la larghezza alla console\n",
    "pd.set_option('display.max_colwidth', None)  # Mostra interamente il contenuto delle celle\n",
    "\n",
    "# Ora puoi visualizzare weights_df nella sua interezza\n",
    "print(weights_df)\n",
    "\n",
    "# Ripristina le opzioni dopo la visualizzazione se desideri tornare alla modalità predefinita\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')\n",
    "pd.reset_option('display.width')\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedura per Calcolare l'Importanza Relativa dei Pesi\n",
    "-Calcolare la somma assoluta dei pesi per ciascun modello.\n",
    "-Dividere ogni peso per la somma assoluta per ottenere la sua importanza relativa.\n",
    "-Convertire in percentuale per rendere più comprensibile l'importanza di ciascuna feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo dell'importanza relativa per ogni modello\n",
    "for model in ['Perceptron', 'Pegasos', 'Logistic_Regression']:\n",
    "    # Calcola la somma assoluta dei pesi per il modello\n",
    "    total_weight = weights_df[model].abs().sum()\n",
    "    \n",
    "    # Calcola l'importanza relativa dei pesi come percentuale\n",
    "    weights_df[f'{model}_importance'] = (weights_df[model].abs() / total_weight) * 100\n",
    "\n",
    "# Visualizza le importanze relative\n",
    "print(weights_df[['Feature', 'Perceptron_importance', 'Pegasos_importance', 'Logistic_Regression_importance']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Progetto_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
